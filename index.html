<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution (ICCV 2023)</title>
    <script type="text/javascript" src="web/latexit.js"></script>
    <script type="text/javascript">
    LatexIT.add('p',true);
    </script>

    <!-- CSS includes -->
    <link href="web/bootstrap.css" rel="stylesheet">
    <link href="web/css.css" rel="stylesheet" type="text/css">
    <link href="web/mystyle.css" rel="stylesheet">

    </head>
<body>

    <div id="header" class="container-fluid">
        <div class="row">
            <h1>MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions <br> for Continuous Space-Time Video Super-Resolution</h1>
            <div class="authors">
                <a href="https://www.linkedin.com/in/hyancheng96" target="_blank">Yan-Cheng Huang</a>, 
                <a href="mailto:yhchen12101@gmail.com", target="_blank">Yi-Hsin Chen</a>, 
                <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng/cv", target="_blank">Wen-Hsiao Peng</a>, 
                <a href="http://acm.cs.nctu.edu.tw/Home.aspx", target="_blank">Ching-Chun Huang</a>
            </div>
        </div>
    
        <p style="text-align:center;">
            <a href="https://www.nycu.edu.tw/en/" target="_blank"><img src="web/NCTU_logo.png" height="100"></a>  â€ƒ
            <a href="http://mapl.nctu.edu.tw/" target="_blank"><img src="web/mapl_logo.png"  height="110"></a>
        </p>
    </div>
    <div class="container" id="abstractdiv">
        <h2>Abstract</h2>
            <p>This work addresses continuous space-time video super-resolution (C-STVSR) that aims to up-scale an input video both spatially and temporally by any scaling factors. One key challenge of C-STVSR is to propagate information temporally among the input video frames. To this end, we introduce a space-time local implicit neural function. It has the striking feature of learning forward motion for a continuum of pixels. We motivate the use of forward motion from the perspective of learning individual motion trajectories, as opposed to learning a mixture of motion trajectories with backward motion. To ease motion interpolation, we encode sparsely sampled forward motion extracted from the input video as the contextual input. Alongwith a reliability-aware splatting and decoding scheme, our framework, termed MoTIF, achieves the state-of-the-art performance on C-STVSR.  </p>
    <!--</div>
    <div class="container" id="intro_video">
        <h2>Introduction Video</h2>
            <p style="text-align:center;">
                <iframe width="1024" height="576" src="https://www.youtube.com/embed/B3KRQmJaLQQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
    </div>-->
    

    <div class="container" id="paperdiv">
        <h2>CICCV 2023 Paper</h2>
        <div class="row">
            <div class="col-sm-3">
            </div>
            <div class="col-sm-3">
                <a href="https://github.com/sichun233746/MoTIF" target="_blank"><p style="text-align: center;">
                <img src="web/icon/github_icon.png">
                <br/>
                Code (Github)
            </p></a></div>
            <div class="col-sm-3">
                <a href="https://arxiv.org/abs/2307.07988" target="_blank"><p style="text-align: center;">
                <img src="web/icon/pdf_icon.png" height="120">
                <br/>
                Paper (arXiv)
            </p></a></div>
            <div class="col-sm-3">
            </div>
        </div>
        <a href="" target="_blank">
            <div class="thumbs">
            </div>
        </a>
        <!--<div>
            <pre class="citation">
            @InProceedings{Chen_2021_CVPR,
                author    = {Huang, Yan-Cheng and Chen, Yi-Hsin and Lu, Cheng-You and Wang, Hui-Po and Peng, Wen-Hsiao and Huang, Ching-Chun},
                title     = {Video Rescaling Networks With Joint Optimization Strategies for Downscaling and Upscaling},
                booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                month     = {June},
                year      = {2021},
                pages     = {3527-3536}
            }
            </pre>
        </div>
    </div>-->

    <div class="container" id="method">
        <h2>Method</h2>
        <p style="text-align:center;">
            <img src="web/arch.png" height="640">
        </p>
        <br>
        <!--<p>
            Overview of the proposed LSTM-VRN and MIMO-VRN for video rescaling. Both schemes involve an invertible network with coupling layers for video downscaling and upscaling. In part (a), LSTM-VRN downscales every video frame $x_t$ independently and forms a prediction $\hat{z}_t$ of the high-frequency component $z_t$ from the LR video frames $\{\hat{y}_i\}_{i=t-L}^{t+L}$ by a bi-directional LSTM that operates in a sliding window manner. In part (b), MIMO-VRN downscales a group of HR video frames $\{x_i\}_{i=t}^{t+g}$ into the LR video frames $\{\hat{y}_i\}_{i=t}^{t+g}$ simultaneously. The upscaling is also done on a group-by-group basis, with the high-frequency components $\{z_i\}_{i=t}^{t+g}$ estimated from the $\{\hat{y}_i\}_{i=t}^{t+g}$ by a predictive module.</p>-->
    </div>

    <div class="container" id="exp_results">
        <h2>Experimental Results (Vid4)</h2>
        <a href="http://mapl.nctu.edu.tw/MIMO_VRN" target="_blank">
            <p style="text-align: center;">
                <img src="web/icon/demo.png" style="width: 15%">
                </br>
                Video Results
            </p>
        </a>
        <a href="./web/vid/Input_x4_Ours.html" target="_blank">Input_x4_Ours</a>
        <a href="./web/vid/Ours_VideoINR.html" target="_blank">Ours_VideoINR</a>
        <a href="./web/vid/GroundTruth_Ours.html" target="_blank">GroundTruth_Ours</a>
        <a href="./web/vid/GroundTruth_VideoINR.html" target="_blank">GroundTruth_VideoINR</a>
    <hr>
    <h3>HR Reconstruction on Vid4</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/table1.png" width="900">
        </div>
        <div class="row" style="text-align: center;">
            <h3><u>Calendar </u></h3>
            <img src="web/pic/vid4_calendar.png" width="900">
        </div>
        <div class="row" style="text-align: center;">
            <h3><u>City </u></h3>
            <img src="web/pic/vid4_city.png" width="900">
        </div>
        <div class="row" style="text-align: center;">
            <h3><u>Walk</u> </h3>
            <img src="web/pic/vid4_walk.png" width="900">
        </div>
        <div class="row" style="text-align: center;">
            <h3><u>Foliage</u> </h3>
            <img src="web/pic/vid4_foliage.png" width="900">
        </div>
    </div>

    <hr>
    <h3>HR Reconstruction on Vimeo-90K-T</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/table2.png" width="500">
        </div>
        <div class="row" style="text-align: center;">
            <img src="web/pic/vimeo.png" width="900">
        </div>
    </div>

    <hr>
    <h3>LR Reconstruction on Vid4</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/pic/lr.png" width="900">
        </div>
        </br>
        <div class="row">
            <p>We still can see the unpleasing artifacts in some of the downscaled LR results especially those produced by SISO downscaling method such as IRN and LSTM-VRN. However, MIMO-VRN can reduce much such artifacts since it utilizes temporal information while downscaling. The other way of reducing such artifacts is to increase the lambda factor in loss function which makes LR video much close to bicubic-downscaled one with a slight decrease in HR reconstrution quality. If the LR quality really bothers you, we suggest training the model with higher lambda factor.</p>
        </div>
        <div class="row" style="text-align: center;">    
            <img src="web/pic/lambda.png" width="400">
        </div>
    </div>

    </div>


    <div id=footer><br></div>
    <!-- Javascript includes -->
    <script src="web/jquery-1.js"></script>
    <script src="web/bootstrap.js"></script>
  

</body></html>
