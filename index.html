<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution (ICCV 2023)</title>
    <script type="text/javascript" src="web/latexit.js"></script>
    <script type="text/javascript">
    LatexIT.add('p',true);
    </script>

    <!-- CSS includes -->
    <link href="web/bootstrap.css" rel="stylesheet">
    <link href="web/css.css" rel="stylesheet" type="text/css">
    <link href="web/mystyle.css" rel="stylesheet">

    </head>
<body>

    <div id="header" class="container-fluid">
        <div class="row">
            <h1>MoTIF: Learning Motion Trajectories with Local Implicit Neural <br> Functions for Continuous Space-Time Video Super-Resolution</h1>
            <div class="authors">
                <a href="mailto:yhchen12101@gmail.com", target="_blank">Yi-Hsin Chen*</a>, 
                <a href="www.linkedin.com/in/sicunchen", target="_blank">Si-Cun Chen*</a>, 
                <a href="https://sites.google.com/site/yylinweb/", target="_blank">Yen-Yu Lin</a>,
                <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng/cv", target="_blank">Wen-Hsiao Peng</a> 
            </div>
        </div>
    
        <p style="text-align:center;">
            <a href="https://www.nycu.edu.tw/en/" target="_blank"><img src="web/NCTU_logo.png" height="100"></a>  â€ƒ
            <a href="http://mapl.nctu.edu.tw/" target="_blank"><img src="web/mapl_logo.png"  height="110"></a>
        </p>
    </div>
    <div class="container" id="abstractdiv">
        <h2>Abstract</h2>
            <p>This work addresses continuous space-time video super-resolution (C-STVSR) that aims to up-scale an input video both spatially and temporally by any scaling factors. One key challenge of C-STVSR is to propagate information temporally among the input video frames. To this end, we introduce a space-time local implicit neural function. It has the striking feature of learning forward motion for a continuum of pixels. We motivate the use of forward motion from the perspective of learning individual motion trajectories, as opposed to learning a mixture of motion trajectories with backward motion. To ease motion interpolation, we encode sparsely sampled forward motion extracted from the input video as the contextual input. Along with a reliability-aware splatting and decoding scheme, our framework, termed MoTIF, achieves the state-of-the-art performance on C-STVSR.  </p>
    <!--</div>
    <div class="container" id="intro_video">
        <h2>Introduction Video</h2>
            <p style="text-align:center;">
                <iframe width="1024" height="576" src="https://www.youtube.com/embed/B3KRQmJaLQQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
    </div>-->
    

    <div class="container" id="paperdiv">
        <h2>ICCV 2023 Paper</h2>
        <div class="row">
            <div class="col-sm-3">
            </div>
            <div class="col-sm-3">
                <a href="https://github.com/sichun233746/MoTIF" target="_blank"><p style="text-align: center;">
                <img src="web/icon/github_icon.png">
                <br/>
                Code (Github)
            </p></a></div>
            <div class="col-sm-3">
                <a href="https://arxiv.org/abs/2307.07988" target="_blank"><p style="text-align: center;">
                <img src="web/icon/pdf_icon.png" height="120">
                <br/>
                Paper (arXiv)
            </p></a></div>
            <div class="col-sm-3">
            </div>
        </div>
        <a href="" target="_blank">
            <div class="thumbs">
            </div>
        </a>
        <!--<div>
            <pre class="citation">
            @InProceedings{Chen_2021_CVPR,
                author    = {Huang, Yan-Cheng and Chen, Yi-Hsin and Lu, Cheng-You and Wang, Hui-Po and Peng, Wen-Hsiao and Huang, Ching-Chun},
                title     = {Video Rescaling Networks With Joint Optimization Strategies for Downscaling and Upscaling},
                booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                month     = {June},
                year      = {2021},
                pages     = {3527-3536}
            }
            </pre>
        </div>
    </div>-->

    <div class="container" id="method">
        <h2>Method</h2>
        <p style="text-align:center;">
            <img src="web/arch.png" height="640">
        </p>
        <br>
        <p>
            Our proposed MoTIF comprises four major components and operates as follows. First, given $I_{0}^L$ and $I_{1}^L$, (1) the encoder $E_I$ converts them into their latent representations $F_{0}^L,F_{1}^L,F_{(0,1)}^L \in \mathbb{R}^{C \times H \times W}$, where $F_{(0,1)}^L$ serves as a rough estimate of the feature of the target frame $I_t^H$. We adopt the off-the-shelf video-based encoder, which fuses information from both $I_{0}^L$ and $I_{1}^L$ in generating $F_{0}^L,F_{1}^L$ and $F_{(0,1)}^L$. Second, (2) the spatial local implicit neural function (S-INF) is queried to super-resolute $F_{0}^L,F_{1}^L$ as $F_{0}^H,F_{1}^H \in \mathbb{R}^{C \times H' \times W'}$, respectively. Third, considering $I_{0}^L$ as sitting at the origin in time, (3) the motion encoder $E_M$ encodes $M_{0 \rightarrow 1}^L \in \mathbb{R}^{2 \times H \times W}$--namely, the forward optical flow map capturing the forward motion from $I_{0}^L$ to $I_{1}^L$--together with its reliability map $Z_{0 \rightarrow 1}^L \in \mathbb{R}^{3 \times H \times W}$ into $T_0^{L} \in \mathbb{R}^{C \times H \times W}$. The optical flow estimation is not always perfect; $Z_{0 \rightarrow 1}^L$ indicates how reliable $M_{0 \rightarrow 1}^L$ is across spatial locations $(x,y)$ (Section~\ref{subsec:estimate_explicit_motion}). Forth, using $T_0^{L}$ as the motion latent, (4) our space-time local implicit neural function (ST-INF) renders a high-resolution, forward motion map $\hat{M}_{0 \rightarrow t}^H \in \mathbb{R}^{2 \times H' \times W'}$ and its reliability map $\hat{Z}_{0 \rightarrow t}^H \in \mathbb{R}^{H' \times W'}$ according to the query space-time coordinates $(x,y,t)$. $\hat{M}_{0 \rightarrow t}^H$ specifies the forward motion of the features in $F_{0}^H$ and is utilized to forward warp $F_{0}^H$ to $F_t^H$. The same motion encoding, rendering and warping processes are repeated for $I_{1}^L$, in aggregating temporally the information from all the reference frames. Lastly, we perform softmax splatting to create $F_t^H$ and $Z_t^H$, which are further combined with $F_{(0,1)}^H$ to decode the high-resolution video frame $\hat{I}_t^H$ at time $t$. $Z_t^H$ indicates how good $F_t^H$ is across spatial locations. It is used to condition the pixel-based decoding of the RGB values from $F_t^H$ and $F_{(0,1)}^H$. 
        </p>
    </div>

    <div class="container" id="exp_results">
        <h2>Experimental Results</h2>
        <!--<a href="http://mapl.nctu.edu.tw/MIMO_VRN" target="_blank">
            <p style="text-align: center;">
                <img src="web/icon/demo.png" style="width: 15%">
                </br>
                Video Results
            </p>
        </a>
        <video width="40%" autoplay="autoplay">
          <source src="./web/vid/Input_X4.webm" type="video/webm">
        </video>
        <video width="40%"  autoplay="autoplay">
          <source src="./web/vid/Ours.webm" type="video/webm">
        </video>-->
    <hr>
    <h3>Video Comparison</h3>
    <hr>
        <img src="web/vid/Input.gif" width="45%">
        <img align="right" src="web/vid/Ours.gif" width="45%">
        <br>
        <a href="./web/vid/Input_x4_Ours.html" target="_blank">Input_x4_Ours</a>
        <br>
        <a href="./web/vid/Ours_VideoINR.html" target="_blank">Ours_VideoINR</a>
        <br>
        <a href="./web/vid/GroundTruth_Ours.html" target="_blank">GroundTruth_Ours</a>
        <br>
        <a href="./web/vid/GroundTruth_VideoINR.html" target="_blank">GroundTruth_VideoINR</a>
    <hr>
    <h3>Quantitative Results - C-STVSR</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/table1.png" width="1200">
        </div>
    </div>

    <hr>
    <h3>Quantitative Results - F-STVSR</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/table2.png" width="1200">
        </div>
    </div>

    <hr>
    <h3>Qualitative  Results</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/pic/cstvsr.png" width="1200">
        </div>
    </div>
    <!--
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/pic/spatial_1.png" width="1200">
        </div>
    </div>
    -->

    <hr>
    <h3>Ablation Studies</h3>
    <hr>
    <div id="comparison_source">
        <div class="row" style="text-align: center;">
            <img src="web/table3.png" height="240">
            <img src="web/table4.png" height="240">
        </div>
    </div>
    </div>



    </div>


    <div id=footer><br></div>
    <!-- Javascript includes -->
    <script src="web/jquery-1.js"></script>
    <script src="web/bootstrap.js"></script>
  

</body></html>
